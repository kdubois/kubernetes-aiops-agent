package dev.kevindubois.rollout.agent;

import io.fabric8.kubernetes.api.model.*;
import io.fabric8.kubernetes.client.KubernetesClient;
import io.quarkus.test.junit.QuarkusTest;
import jakarta.inject.Inject;
import jakarta.ws.rs.core.Response;
import dev.kevindubois.rollout.agent.a2a.KubernetesAgentResource;
import dev.kevindubois.rollout.agent.model.KubernetesAgentRequest;
import dev.kevindubois.rollout.agent.model.KubernetesAgentResponse;
import org.junit.jupiter.api.*;
import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;

import java.util.*;
import java.util.concurrent.TimeUnit;

import static org.junit.jupiter.api.Assertions.*;

/**
 * Comprehensive End-to-End Integration Test for Kubernetes Agent
 * 
 * This test simulates a complete workflow:
 * 1. Sets up a pod in Kubernetes with error logs (database connection failure)
 * 2. Simulates the Argo Rollouts plugin calling the agent
 * 3. Verifies the agent analyzes logs correctly
 * 4. Verifies the agent creates a GitHub PR with a fix
 * 
 * Requirements:
 * - OPENAI_API_KEY environment variable must be set
 * - GITHUB_TOKEN environment variable must be set for PR creation
 * - Kubernetes cluster must be accessible (in-cluster or via kubeconfig)
 * - Test namespace will be created and cleaned up automatically
 */
@QuarkusTest
@EnabledIfEnvironmentVariable(named = "OPENAI_API_KEY", matches = ".*")
@TestMethodOrder(MethodOrderer.OrderAnnotation.class)
class ComprehensiveE2ETest {
    
    private static final String TEST_NAMESPACE = "k8s-agent-e2e-test";
    private static final String TEST_APP_NAME = "failing-app";
    private static final String TEST_POD_NAME = TEST_APP_NAME + "-pod";
    private static final String CANARY_VERSION = "v2.1.0";
    private static final String STABLE_VERSION = "v2.0.0";
    
    @Inject
    KubernetesClient k8sClient;
    
    @Inject
    KubernetesAgentResource agentResource;
    
    private static boolean namespaceCreated = false;
    private static String createdPodName = null;
    
    /**
     * Setup: Create test namespace and deploy a failing pod
     */
    @BeforeAll
    static void setupTestEnvironment() {
        System.out.println("=== Setting up E2E Test Environment ===");
        
        // Note: We'll create the namespace in the first test method since @BeforeAll
        // cannot use injected dependencies in JUnit 5
        System.out.println("Test environment setup will be done in first test");
    }
    
    /**
     * Cleanup before first test only to ensure clean state from previous runs
     */
    @BeforeEach
    void cleanupBeforeTest() {
        // Only cleanup before the first test (test1)
        // Don't cleanup between tests as they depend on the pod existing
        if (k8sClient == null || createdPodName != null) {
            return;
        }
        
        try {
            // Delete pod if it exists from a previous test run
            Pod existingPod = k8sClient.pods()
                .inNamespace(TEST_NAMESPACE)
                .withName(TEST_POD_NAME)
                .get();
            
            if (existingPod != null) {
                System.out.println("üßπ Cleaning up existing pod from previous test run...");
                k8sClient.pods()
                    .inNamespace(TEST_NAMESPACE)
                    .withName(TEST_POD_NAME)
                    .delete();
                
                // Wait for deletion
                for (int i = 0; i < 30; i++) {
                    TimeUnit.SECONDS.sleep(1);
                    Pod pod = k8sClient.pods()
                        .inNamespace(TEST_NAMESPACE)
                        .withName(TEST_POD_NAME)
                        .get();
                    if (pod == null) {
                        System.out.println("‚úÖ Cleanup complete");
                        break;
                    }
                }
            }
        } catch (Exception e) {
            System.err.println("‚ö†Ô∏è  Cleanup warning: " + e.getMessage());
        }
    }
    
    /**
     * Helper method to ensure namespace exists
     */
    private void ensureNamespaceExists() {
        if (namespaceCreated) {
            return;
        }
        
        System.out.println("Creating test namespace: " + TEST_NAMESPACE);
        try {
            Namespace namespace = new NamespaceBuilder()
                .withNewMetadata()
                    .withName(TEST_NAMESPACE)
                    .addToLabels("test", "k8s-agent-e2e")
                    .addToLabels("created-by", "comprehensive-e2e-test")
                .endMetadata()
                .build();
            
            k8sClient.namespaces().resource(namespace).create();
            namespaceCreated = true;
            System.out.println("‚úÖ Created test namespace: " + TEST_NAMESPACE);
        } catch (Exception e) {
            System.err.println("‚ö†Ô∏è  Namespace might already exist: " + e.getMessage());
        }
        
        // Wait for namespace to be ready
        try {
            TimeUnit.SECONDS.sleep(2);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
    }
    /**
     * Test 1: Create a failing pod with database connection errors
     */
    @Test
    @Order(1)
    void test1_createFailingPod() {
        System.out.println("\n=== Test 1: Creating Failing Pod ===");
        
        // Ensure namespace exists
        ensureNamespaceExists();
        
        
        // Create a pod that will fail due to incorrect database configuration
        Pod failingPod = new PodBuilder()
            .withNewMetadata()
                .withName(TEST_POD_NAME)
                .withNamespace(TEST_NAMESPACE)
                .addToLabels("app", TEST_APP_NAME)
                .addToLabels("version", CANARY_VERSION)
                .addToLabels("role", "canary")
                .addToLabels("rollout", "demo-rollout")
            .endMetadata()
            .withNewSpec()
                .addNewContainer()
                    .withName("app")
                    .withImage("busybox:latest")
                    .withCommand("/bin/sh", "-c")
                    .withArgs(
                        "echo '2024-01-15T10:30:45Z INFO Starting application version " + CANARY_VERSION + "' && " +
                        "echo '2024-01-15T10:30:45Z INFO Connecting to database...' && " +
                        "echo '2024-01-15T10:30:45Z INFO Using database host from env: localhost' && " +
                        "echo '2024-01-15T10:30:45Z ERROR Failed to connect to database' && " +
                        "echo '2024-01-15T10:30:45Z ERROR Connection refused: localhost:5432' && " +
                        "echo '2024-01-15T10:30:45Z ERROR Database connection pool initialization failed' && " +
                        "echo '2024-01-15T10:30:45Z FATAL Application startup failed' && " +
                        "echo '2024-01-15T10:30:45Z ERROR Exit code: 1' && " +
                        "exit 1"
                    )
                    .addNewEnv()
                        .withName("DATABASE_HOST")
                        .withValue("localhost")
                    .endEnv()
                    .addNewEnv()
                        .withName("DATABASE_PORT")
                        .withValue("5432")
                    .endEnv()
                    .addNewEnv()
                        .withName("APP_VERSION")
                        .withValue(CANARY_VERSION)
                    .endEnv()
                    .withNewResources()
                        .addToRequests("memory", new Quantity("64Mi"))
                        .addToRequests("cpu", new Quantity("100m"))
                        .addToLimits("memory", new Quantity("128Mi"))
                        .addToLimits("cpu", new Quantity("200m"))
                    .endResources()
                .endContainer()
                .withRestartPolicy("Always")
            .endSpec()
            .build();
        
        try {
            Pod created = k8sClient.pods()
                .inNamespace(TEST_NAMESPACE)
                .resource(failingPod)
                .create();
            
            createdPodName = created.getMetadata().getName();
            System.out.println("‚úÖ Created failing pod: " + createdPodName);
            
            // Wait for pod to fail at least once
            System.out.println("‚è≥ Waiting for pod to fail (up to 30 seconds)...");
            boolean podFailed = false;
            for (int i = 0; i < 30; i++) {
                TimeUnit.SECONDS.sleep(1);
                Pod pod = k8sClient.pods()
                    .inNamespace(TEST_NAMESPACE)
                    .withName(createdPodName)
                    .get();
                
                if (pod != null && pod.getStatus() != null && 
                    pod.getStatus().getContainerStatuses() != null) {
                    for (ContainerStatus cs : pod.getStatus().getContainerStatuses()) {
                        if (cs.getRestartCount() != null && cs.getRestartCount() > 0) {
                            podFailed = true;
                            System.out.println("‚úÖ Pod has failed and restarted " + 
                                cs.getRestartCount() + " time(s)");
                            break;
                        }
                    }
                }
                if (podFailed) break;
            }
            
            assertTrue(podFailed, "Pod should have failed at least once");
            
        } catch (Exception e) {
            fail("Failed to create test pod: " + e.getMessage());
        }
    }
    
    /**
     * Test 2: Simulate Argo Rollouts plugin calling the agent
     */
    @Test
    @Order(2)
    void test2_simulateArgoRolloutsPluginCall() {
        System.out.println("\n=== Test 2: Simulating Argo Rollouts Plugin Call ===");
        
        assertNotNull(createdPodName, "Pod should have been created in previous test");
        
        // Build request context similar to what Argo Rollouts plugin would send
        Map<String, Object> context = new HashMap<>();
        context.put("namespace", TEST_NAMESPACE);
        context.put("podName", createdPodName);
        context.put("rolloutName", "demo-rollout");
        context.put("canaryVersion", CANARY_VERSION);
        context.put("stableVersion", STABLE_VERSION);
        context.put("failureReason", "CrashLoopBackOff");
        context.put("errorRate", 0.95);
        
        // Add repository URL for PR creation (use a test repo or mock)
        String repoUrl = System.getenv("TEST_GITHUB_REPO");
        if (repoUrl == null || repoUrl.isEmpty()) {
            repoUrl = "https://github.com/kdubois/rollouts-demo";
            System.out.println("‚ö†Ô∏è  TEST_GITHUB_REPO not set, using default: " + repoUrl);
        }
        context.put("repoUrl", repoUrl);
        
        // Add analysis metrics as individual context values
        // (Qute templates don't support nested map object literals)
        context.put("successRate", 0.05);
        context.put("errorCount", 47);
        context.put("totalRequests", 50);
        
        // Create the request - explicitly frame as a code bug to trigger PR creation
        KubernetesAgentRequest request = new KubernetesAgentRequest(
            "argo-rollouts-plugin",
            "A canary deployment is failing with database connection errors. " +
            "Please analyze the pod logs and events, identify the root cause, " +
            "and create a GitHub PR with a code fix. " +
            "The application code has a hardcoded 'localhost' value that needs to be changed " +
            "to read from an environment variable. This is a CODE BUG that requires a PR fix. " +
            "Create a PR that fixes the hardcoded database host in the application code.",
            context
        );
        
        System.out.println("üì§ Sending analysis request to agent...");
        System.out.println("   Namespace: " + TEST_NAMESPACE);
        System.out.println("   Pod: " + createdPodName);
        System.out.println("   Repo: " + repoUrl);
        
        // Call the agent (this may take 30-60 seconds)
        long startTime = System.currentTimeMillis();
        Response response = agentResource.analyze(request);
        long duration = System.currentTimeMillis() - startTime;
        
        System.out.println("‚úÖ Agent responded in " + duration + "ms");
        
        // Verify response
        assertNotNull(response, "Response should not be null");
        assertEquals(Response.Status.OK.getStatusCode(), response.getStatus(),
            "Response should be OK");
        
        KubernetesAgentResponse agentResponse = (KubernetesAgentResponse) response.getEntity();
        assertNotNull(agentResponse, "Response body should not be null");
        
        // Store for next test
        System.out.println("\nüìã Agent Analysis Results:");
        System.out.println("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ");
        System.out.println("Analysis: " + agentResponse.analysis());
        System.out.println("\nRoot Cause: " + agentResponse.rootCause());
        System.out.println("\nRemediation: " + agentResponse.remediation());
        System.out.println("\nPromote Canary: " + agentResponse.promote());
        System.out.println("Confidence: " + agentResponse.confidence() + "%");
        if (agentResponse.prLink() != null) {
            System.out.println("\nüîó PR Link: " + agentResponse.prLink());
        }
        System.out.println("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ");
    }
    
    /**
     * Test 3: Verify agent analyzed logs correctly
     */
    @Test
    @Order(3)
    void test3_verifyLogAnalysis() {
        System.out.println("\n=== Test 3: Verifying Log Analysis ===");
        
        assertNotNull(createdPodName, "Pod should have been created");
        
        // Create request for analysis verification
        Map<String, Object> context = new HashMap<>();
        context.put("namespace", TEST_NAMESPACE);
        context.put("podName", createdPodName);
        context.put("rolloutName", "demo-rollout");
        
        KubernetesAgentRequest request = new KubernetesAgentRequest(
            "test-user",
            "Analyze the pod failure and identify the root cause",
            context
        );
        
        Response response = agentResource.analyze(request);
        KubernetesAgentResponse agentResponse = (KubernetesAgentResponse) response.getEntity();
        
        // Verify analysis contains key information
        assertNotNull(agentResponse.analysis(), "Analysis should not be null");
        assertNotNull(agentResponse.rootCause(), "Root cause should be identified");
        assertNotNull(agentResponse.remediation(), "Remediation should be provided");
        
        String analysis = agentResponse.analysis().toLowerCase();
        String rootCause = agentResponse.rootCause().toLowerCase();
        
        // Verify the agent identified the database connection issue
        boolean mentionsDatabase = analysis.contains("database") || 
                                   rootCause.contains("database") ||
                                   analysis.contains("connection") ||
                                   rootCause.contains("connection");
        
        assertTrue(mentionsDatabase, 
            "Analysis should mention database or connection issues. " +
            "Analysis: " + agentResponse.analysis() + 
            ", Root Cause: " + agentResponse.rootCause());
        
        // Verify confidence score is reasonable
        assertTrue(agentResponse.confidence() >= 0 && agentResponse.confidence() <= 100,
            "Confidence should be between 0 and 100");
        
        // For a clear failure like this, confidence should be relatively high
        assertTrue(agentResponse.confidence() >= 50,
            "Confidence should be at least 50% for a clear database connection failure");
        
        // Verify promotion decision (should not promote a failing canary)
        assertFalse(agentResponse.promote(),
            "Agent should recommend NOT promoting a failing canary deployment");
        
        System.out.println("‚úÖ Log analysis verification passed");
        System.out.println("   - Database/connection issue identified: ‚úì");
        System.out.println("   - Confidence score: " + agentResponse.confidence() + "%");
        System.out.println("   - Promotion decision: " + (agentResponse.promote() ? "PROMOTE" : "DO NOT PROMOTE"));
    }
    
    /**
     * Test 4: Verify GitHub PR creation (if GITHUB_TOKEN is set)
     */
    @Test
    @Order(4)
    @EnabledIfEnvironmentVariable(named = "GITHUB_TOKEN", matches = ".*")
    void test4_verifyGitHubPRCreation() {
        System.out.println("\n=== Test 4: Verifying GitHub PR Creation ===");
        
        String githubToken = System.getenv("GITHUB_TOKEN");
        String testRepo = System.getenv("TEST_GITHUB_REPO");
        
        if (testRepo == null || testRepo.isEmpty()) {
            testRepo = "https://github.com/kdubois/rollouts-demo";
            System.out.println("‚ö†Ô∏è  TEST_GITHUB_REPO not set, using default: " + testRepo);
        }
        
        assertNotNull(createdPodName, "Pod should have been created");
        
        // Create request that explicitly asks for PR creation
        Map<String, Object> context = new HashMap<>();
        context.put("namespace", TEST_NAMESPACE);
        context.put("podName", createdPodName);
        context.put("rolloutName", "demo-rollout");
        context.put("repoUrl", testRepo);
        context.put("canaryVersion", CANARY_VERSION);
        context.put("stableVersion", STABLE_VERSION);
        
        KubernetesAgentRequest request = new KubernetesAgentRequest(
            "test-user",
            "Analyze the database connection failure and create a GitHub PR with a fix. " +
            "The issue is that DATABASE_HOST is set to 'localhost' but should be set to " +
            "the actual database service name.",
            context
        );
        
        System.out.println("üì§ Requesting PR creation for repo: " + testRepo);
        
        Response response = agentResource.analyze(request);
        KubernetesAgentResponse agentResponse = (KubernetesAgentResponse) response.getEntity();
        
        // Check if PR was created
        if (agentResponse.hasPrLink()) {
            System.out.println("‚úÖ GitHub PR created successfully!");
            System.out.println("   PR URL: " + agentResponse.prLink());
            
            assertNotNull(agentResponse.prLink(), "PR link should not be null");
            assertTrue(agentResponse.prLink().startsWith("https://github.com/"),
                "PR link should be a valid GitHub URL");
            
        } else {
            System.out.println("‚ÑπÔ∏è  No PR was created. This might be expected if:");
            System.out.println("   - The agent determined a PR wasn't necessary");
            System.out.println("   - The repository doesn't exist or isn't accessible");
            System.out.println("   - The fix doesn't require code changes");
            System.out.println("\n   Analysis: " + agentResponse.analysis());
        }
    }
    
    /**
     * Test 5: Verify agent can handle multiple analysis requests (memory test)
     */
    @Test
    @Order(5)
    void test5_verifyMemoryAndMultipleRequests() {
        System.out.println("\n=== Test 5: Testing Memory and Multiple Requests ===");
        
        assertNotNull(createdPodName, "Pod should have been created");
        
        String memoryId = "e2e-test-session-" + System.currentTimeMillis();
        
        // First request
        Map<String, Object> context1 = new HashMap<>();
        context1.put("namespace", TEST_NAMESPACE);
        context1.put("podName", createdPodName);
        
        KubernetesAgentRequest request1 = new KubernetesAgentRequest(
            "test-user",
            "What is the status of the pod?",
            context1,
            memoryId
        );
        
        Response response1 = agentResource.analyze(request1);
        KubernetesAgentResponse agentResponse1 = (KubernetesAgentResponse) response1.getEntity();
        
        assertNotNull(agentResponse1);
        System.out.println("‚úÖ First request completed");
        
        // Second request (should have memory of first)
        Map<String, Object> context2 = new HashMap<>();
        context2.put("namespace", TEST_NAMESPACE);
        
        KubernetesAgentRequest request2 = new KubernetesAgentRequest(
            "test-user",
            "Based on what you found, what is the root cause?",
            context2,
            memoryId
        );
        
        Response response2 = agentResource.analyze(request2);
        KubernetesAgentResponse agentResponse2 = (KubernetesAgentResponse) response2.getEntity();
        
        assertNotNull(agentResponse2);
        assertNotNull(agentResponse2.rootCause());
        
        System.out.println("‚úÖ Second request completed with memory context");
        System.out.println("   Root cause from memory: " + agentResponse2.rootCause());
    }
    
    /**
     * Cleanup after all tests complete to ensure resources are deleted
     */
    @AfterEach
    void cleanupAfterTest() {
        // Don't cleanup after each test - tests depend on the pod existing
        // Cleanup will happen in @AfterAll
    }
    
    /**
     * Cleanup: Delete test pod and namespace after all tests
     */
    @AfterAll
    static void cleanupTestEnvironment() {
        System.out.println("\n=== Cleaning up E2E Test Environment ===");
        System.out.println("Note: Namespace cleanup will be done manually or via test framework");
        System.out.println("To manually cleanup, run: kubectl delete namespace " + TEST_NAMESPACE);
        System.out.println("=== E2E Test Cleanup Complete ===\n");
        
        // Reset static variables for next test run
        createdPodName = null;
        namespaceCreated = false;
    }
}

